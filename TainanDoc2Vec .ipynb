{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = Doc2Vec(contentcutlist, size=100, window=8, min_count=5, workers=4)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "#改為中文繁體字典,預設是簡體，可到jieba的GitHub下載\n",
    "jieba.set_dictionary(r'.\\data\\jieba_dict.txt.big') \n",
    "from wordcloud import WordCloud\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\".\\data\\bigtable_1.3.json\") as f:\n",
    "    BigAnalyzeTable=json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StopWdFilter(user_input, stop_words):\n",
    "    \"\"\"Sanitize using intersection and list.remove()\"\"\"\n",
    "    # Downsides:\n",
    "    #   - Looping over list while removing from it?\n",
    "    #     http://stackoverflow.com/questions/1207406/remove-items-from-a-list-while-iterating-in-python\n",
    "    stop_words = set()\n",
    "    with open('./dict/stopwords.txt', 'r', encoding='utf-8') as sw:\n",
    "        for line in sw:\n",
    "            stop_words.add(line.strip('\\n'))\n",
    "    for sw in stop_words.intersection(user_input):\n",
    "        while sw in user_input:\n",
    "            user_input.remove(sw)\n",
    "\n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CutWithFilter(CorpusList):\n",
    "    result=[]\n",
    "    for y in [jieba.cut(x) for x in CorpusList]:\n",
    "        content = StopWdFilter([ _ for _ in y], stopwords)\n",
    "        result.append(content)\n",
    "    return result\n",
    "#所以我選擇所過濾的是跟「文意」比較無關的符號\n",
    "stopwords = ['〔','〕','／','（','）','「','」','『','』','\\n','nan']\n",
    "#像下面這種比較是傳統文字探勘在過濾字詞可能過濾的一些字詞\n",
    "#stopwords = ['〔','〕','／','（','）','「','」','『','』','的','就是','這個','可以','一定'\\\n",
    "#             ,'一個','nan','沒有','我們','不是','不用','不是','自己','只要','大家','覺得','因為'\\\n",
    "#             ,'對於','起來','如果','每個','很多','一樣','時候','加上','使用','只是']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#只留下店名跟文章做分類分析\n",
    "FoodArticle=[{'name':dien[\"name\"],'content':\"\".join(dien['contentcut'].split(\" \"))} for dien in BigAnalyzeTable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FoodArticle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Food_df=pd.DataFrame(FoodArticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Food_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Food = Food_df.name+ \" \" + Food_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Food_List = CutWithFilter(Food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Food_List[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 把gensim, Doc2Vec的套件匯入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "Lsent = gensim.models.doc2vec.LabeledSentence\n",
    "def labelizeNews(food, label_type):\n",
    "    labelized = []\n",
    "    #enumertate include index\n",
    "    for i, v in enumerate(food):\n",
    "        label = '%s_%s' %(label_type,i)\n",
    "        labelized.append(Lsent(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "\n",
    "# for i,v in enumerate()\n",
    "# i為流水號，1  2  3\n",
    "# v為每篇文章的，第1個字全部  第2個字全部  第3個字全部\n",
    "# label_type為type標籤\n",
    "# labelized = []\n",
    "# for i,v in enumerate(Food_List):\n",
    "#     label = '%s_%s' %('Food',i)\n",
    "#     labelized.append(Lsent(v, [label]))\n",
    "\n",
    "# 將第一篇的文字全部上Food_1的標籤\n",
    "# 將第二篇的文字全部上Food_2的標籤\n",
    "# 以此類推"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 將每個的文章給一個標籤，方便分析完對應"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_Food = labelizeNews(Food_List,'Food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Train_Food))\n",
    "print(type(Train_Food[1]))\n",
    "Train_Food[0]#為上完標籤的gensimlabel＿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定Doc2Vec參數 (使用seed or save model 來排除訓練的隨機成分，如果需要每次的向量結果相同 每一個Corpus產出400維的向量，預設使用3個核心做運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 400   \n",
    "model_dm = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5, workers=3)\n",
    "model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, size=size, sample=1e-3, negative=5,dm=0, workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 由有順序的句子建立model詞彙(讓model讀懂用的) 輸入的資料必須是((文章的有順序斷詞list)的list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dm.build_vocab(Train_Food)\n",
    "model_dbow.build_vocab(Train_Food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練十次，每次隨機交換文章順序 (由於是隨機, 要每次結果完全相同也可以固定訓練次序)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    #總共幾篇文章(假設225) 對1~225做隨機排序的array\n",
    "    perm = np.random.permutation(len(Train_Food))\n",
    "    \n",
    "    #將所有的文章隨機順序丟進去訓練 並告知model 共有225個example  5種時?\n",
    "    model_dm.train([Train_Food[x] for x in perm],total_examples=model_dm.corpus_count,epochs=model_dm.iter)\n",
    "    model_dbow.train([Train_Food[x] for x in perm],total_examples=model_dbow.corpus_count,epochs=model_dbow.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可以把模型存起來，下次直接取回向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dm.save('./data/doc2verData/model_dm')\n",
    "model_dbow.save('./data/doc2verData/model_dbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出文章向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVecs(model, corpus, size):\n",
    "    #取出每一個tag(Eat_1.Eat_2...Ser_1.Ser_2....)的向量(docvecs)，並將向量長度全都變成400size\n",
    "    vecs = [model.docvecs[z.tags[0]].reshape((1, size)) for z in corpus]\n",
    "    \n",
    "    return np.concatenate(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.concatenate(model_dm.docvecs['Food_3'].reshape((1, 400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取出兩個模型 dm, dbow 取得其兩個模型的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_Food_vecs_dm = getVecs(model_dm, Train_Food, size)\n",
    "Train_Food_vecs_dbow = getVecs(model_dbow, Train_Food, size)\n",
    "#將兩種模型的vec上下疊加\n",
    "Food_train_vecs = np.hstack((Train_Food_vecs_dm, Train_Food_vecs_dbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入scikit learn 套件 kmeans; 並將Doc2Vec的向量做分群 將所有食記嘗試分50群，並人工對群體做觀察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "Ngroup=50\n",
    "#固定random state 隨機初始值 fixed the seed\n",
    "Food_kmeans = KMeans(n_clusters=Ngroup, random_state=0).fit(Food_train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#清一下記憶體\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將分群結果塞回DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Food_df\n",
    "# Food_List\n",
    "\n",
    "#移除,的方法\n",
    "Food_df['name']=Food_df['name'].apply(lambda y:y.replace(',',\"\"))\n",
    "Food_df['name']=Food_df['name'].apply(lambda y:y.replace('\\n',\"\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Food_df['Cluster'] = Food_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Food_df.to_excel('./data/doc2verData/Food_'+str(Ngroup)+'gp.xlsx')\n",
    "Food_df.to_csv('./data/doc2verData/Food_'+str(Ngroup)+'gp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 試畫群組文字雲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WC_Str=''\n",
    "# for i, x in enumerate(Food_kmeans.labels_):\n",
    "#     if x == 2:\n",
    "#         WC_Str += ' '.join(Food_List[i])\n",
    "\n",
    "# ww = WordCloud(max_font_size=100, relative_scaling=1,max_words=40,width=800, height=600,background_color='white',random_state=0,\\\n",
    "#                stopwords=[\"臺南\",\"菜色\",\"味道\",\"真的\",\"店家\",\"這道\",\"感覺\",\"比較\",\"用餐\",\"搭配\",\n",
    "#          \"料理\",\"空間\",\"一個\",\"地方\",\"台南市\",\"台南\",\"口味\",\"店員\",'一起','一段',\n",
    "#          '部分','餐點','地方','覺得','廁所','二樓',\"看到\",\"市場\",\"地址\",\"三段\",\"一段\",\n",
    "#          '兩段','中西區','海安','一盒','會館','座位','選擇','下午','提供','制式','使用',\n",
    "#          '北區','營業時間','電話','美食','一份','朋友','東西','一下','來到','一些','二段',\n",
    "#          '有點','應該','老闆','推薦','這家','一直','菜單','想法','平路','府連',\n",
    "#          '喜歡','東區','整個','時間','店內','國華','台北','聽說','升級','套餐',\n",
    "#          '餐廳','相當','店內','裡面','一點','一口','知道','已經','這是','客人'])\\\n",
    "#                .generate(WC_Str)\n",
    "# ww.font_path = 'C:\\Windows\\Fonts\\msjhbd' #微軟正黑體的路徑；字體可以自己設定，預設是英文的遇到中文會讀不出來\n",
    "# ww.to_image()\n",
    "# #ww.to_file('./pic/50gp_5.jpg') 可輸出圖檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/doc2verData/Food_'+str(Ngroup)+'gp.csv') as f:\n",
    "    x=f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 發現有些群體過小 將它們合併為一個，並將超少評論或無評的文章群做標記"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfood=[dien.replace('\\n',\"\").split(',')[2:] for dien in x[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups={}\n",
    "for dien in kfood:\n",
    "    if dien[1] not in groups:\n",
    "        groups[dien[1]]=set()\n",
    "    groups[dien[1]].add(dien[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups['50']=set()\n",
    "willdel=set()\n",
    "for group in groups:\n",
    "    if len(groups[group])<40:\n",
    "        groups['50'].update(groups[group])\n",
    "        willdel.add(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for group in willdel:\n",
    "    del groups[group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=0\n",
    "newgroups={}\n",
    "for i in groups:\n",
    "    if i ==\"0\":\n",
    "        newgroups['無評']=list(groups[i])\n",
    "    elif i ==\"50\":\n",
    "        newgroups['無分類']=list(groups[i])\n",
    "    else:\n",
    "        g+=1\n",
    "        newgroups[str(g)]=list(groups[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(newgroups))\n",
    "print([len(newgroups[i]) for i in newgroups])\n",
    "#為了存成json 轉格式\n",
    "# x=[dict({i:newgroups[i]}) for i in newgroups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/doc2verData/Food_kmeans.json','w') as f:\n",
    "    json.dump(newgroups,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
